{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML related\n",
    "from time import sleep, thread_time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# system requirements\n",
    "import os, sys\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "path = os.getcwd()\n",
    "all_files = glob.glob(os.path.join(path, \"./labeled/*_label.csv\"))\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "\n",
    "# Drop columns\n",
    "df = df.drop(columns=[\"timestamp\"]) # drop timestamp column\n",
    "class_x = df.drop(columns=[\"class\"])\n",
    "class_y = df[\"class\"]\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    class_x,\n",
    "    class_y,\n",
    "    test_size=0.3, # 70% training and 30% test\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "df_analysis = pd.concat([x_test, y_test], axis=1) # this print the test data with the class\n",
    "print(df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. RandomForest Model Test Results\n",
    "import pickle\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = OneVsRestClassifier(RandomForestClassifier(random_state=1))\n",
    "\n",
    "random_forest.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GB Model Test Results\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = OneVsRestClassifier(\n",
    "    GradientBoostingClassifier(\n",
    "        random_state=1,\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "    )\n",
    ")\n",
    "gb.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model Test Results\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = OneVsRestClassifier(\n",
    "    LogisticRegression(\n",
    "        random_state=1,\n",
    "        max_iter=10000,\n",
    "        solver=\"lbfgs\",\n",
    "    )\n",
    ")\n",
    "logistic.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Model Test Results\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive_bayes = OneVsRestClassifier(\n",
    "    GaussianNB()\n",
    ")\n",
    "naive_bayes.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Stacking Model Test Results\n",
    "# change final_estimator to minimum 2 models\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "stacking = OneVsRestClassifier(\n",
    "    StackingClassifier(\n",
    "        estimators=[\n",
    "            (\"rf\", random_forest),\n",
    "            (\"gb\", gb),\n",
    "            (\"logistic\", logistic),\n",
    "            (\"naive_bayes\", naive_bayes),\n",
    "        ],\n",
    "        final_estimator=MLPClassifier(\n",
    "            random_state=1,\n",
    "            max_iter=10000,\n",
    "            hidden_layer_sizes=(100, 100),\n",
    "            solver=\"lbfgs\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "stacking.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, warnings\n",
    "from turtle import color, title\n",
    "\n",
    "import pickle  # dump model\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "FIG_SIZE = 7.5\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, title, labels, color, normalize=False):\n",
    "    if normalize == False:\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test,\n",
    "            y_pred,\n",
    "            labels=labels,\n",
    "            cmap=color,\n",
    "            xticks_rotation=\"vertical\",\n",
    "        )\n",
    "\n",
    "        return\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        labels=labels,\n",
    "        cmap=color,\n",
    "        normalize=\"true\",\n",
    "        xticks_rotation=\"vertical\",\n",
    "    )\n",
    "    return\n",
    "    # cm.plot()\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append((\"rf\", random_forest))\n",
    "models.append((\"naive_bayes\", naive_bayes))\n",
    "models.append((\"stacking\", stacking))\n",
    "models.append((\"logistic\", logistic))\n",
    "models.append((\"gb\", gb))\n",
    "\n",
    "\n",
    "models_name = [\n",
    "    \"randomForest\",\n",
    "    \"naive_bayes\",\n",
    "    \"stacking\",\n",
    "    \"logistic\",\n",
    "    \"gradientBoost\",\n",
    "]\n",
    "labels = [\"Normal\", \"Command Injection TL\", \"Command Injection TH\", \"Dos\"]\n",
    "\n",
    "model_test_accuracy = []\n",
    "model_test_precision = []\n",
    "model_test_recall = []\n",
    "model_test_f1 = []\n",
    "\n",
    "\n",
    "for x, y in models:\n",
    "    train_start = time.time()\n",
    "    y.fit(x_train, y_train)\n",
    "    train_finished = time.time()\n",
    "    pred_start = time.time()\n",
    "    y_pred = y.predict(x_test)\n",
    "    pred_stop = time.time()\n",
    "    y_pred_proba = y.predict_proba(x_test)[:, 1]\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = sklearn.metrics.precision_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = sklearn.metrics.recall_score(y_test, y_pred, average=\"macro\")\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(\n",
    "        y_test, y_pred, average=\"macro\", labels=np.unique(y_pred)\n",
    "    )\n",
    "\n",
    "    # append algorithm overall test results to list\n",
    "    model_test_accuracy.append(accuracy)\n",
    "    model_test_precision.append(precision)\n",
    "    model_test_recall.append(recall)\n",
    "    model_test_f1.append(f1)\n",
    "    sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # dump model\n",
    "    pickle.dump(y, open(\"models/\" + x + \".pkl\", \"wb\"))\n",
    "\n",
    "    print(\n",
    "        \"\"\"\n",
    "        Model: %s \n",
    "        Dataset size: %s\n",
    "        Accuracy: %f\n",
    "        Precision: %f\n",
    "        Recall: %f\n",
    "        F1: %f\n",
    "        Train time: %s\n",
    "        Predict time: %s\n",
    "        \"\"\"\n",
    "        % (\n",
    "            x,\n",
    "            len(df),\n",
    "            accuracy,\n",
    "            precision,\n",
    "            recall,\n",
    "            f1,\n",
    "            train_finished - train_start,\n",
    "            pred_stop - pred_start,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # overall plot_confusion_matrix\n",
    "    plot_confusion_matrix(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        x,\n",
    "        labels,\n",
    "        plt.cm.Blues,\n",
    "    )\n",
    "\n",
    "# plot dash graph\n",
    "fig, axes = plt.subplots(figsize=(FIG_SIZE, FIG_SIZE))\n",
    "axes.title.set_text(\"Accuracy of Models\")\n",
    "axes.plot(\n",
    "    models_name,\n",
    "    model_test_accuracy,\n",
    "    label=\"Accuracy\",\n",
    "    marker=\"o\",\n",
    "    color=\"blue\",\n",
    "    linestyle=\"dashed\",\n",
    ")\n",
    "\n",
    "\n",
    "# plot histogram\n",
    "fig, axes = plt.subplots(figsize=(FIG_SIZE, FIG_SIZE))\n",
    "axes.bar(models_name, model_test_accuracy)\n",
    "axes.set_title(\"Accuracy\")\n",
    "axes.set_ylabel(\"Accuracy\")\n",
    "axes.set_xlabel(\"Algorithms\")\n",
    "axes.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# float a pie chart\n",
    "fig, axes = plt.subplots(figsize=(FIG_SIZE, FIG_SIZE))\n",
    "axes.pie(\n",
    "    model_test_accuracy,\n",
    "    labels=models_name,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    ")\n",
    "axes.grid(True)\n",
    "axes.set_title(\"Accuracy Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all confirm attacks\n",
    "\n",
    "import glob, pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "all_files = glob.glob(os.path.join(path, \"./labeled/*_label.csv\"))\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "\n",
    "\n",
    "count = 0\n",
    "file_name = \"anomaly.csv\"\n",
    "\n",
    "# remove if the file exists\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "\n",
    "\n",
    "# save to new file if condition is met\n",
    "with open(file_name, \"w\") as f:\n",
    "    f.write(\n",
    "        str(list(df.columns)[1:][:-1])\n",
    "        .replace(\"[\", \"\")\n",
    "        .replace(\"]\", \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\" \", \"\"),\n",
    "    )\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "\n",
    "    if str(df[\"class\"][i]) != \"Normal\":\n",
    "\n",
    "        with open(\"anomaly.csv\", \"a\") as f:\n",
    "            f.write(\n",
    "                str(df[\"tank_liquidlevel\"][i])\n",
    "                + \",\"\n",
    "                + str(df[\"flowlevel\"][i])\n",
    "                + \",\"\n",
    "                + str(df[\"bottle_liquidlevel\"][i])\n",
    "                + \",\"\n",
    "                + str(df[\"motor_status\"][i]),\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            count += 1\n",
    "\n",
    "print(\"total number of atacks from logs: \", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tank_liquidlevel\", \"flowlevel\", \"bottle_liquidlevel\", \"motor_status\"]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def get_result(\n",
    "    model_name, tank_liquidlevel, flowlevel, bottle_liquidlevel, motor_status\n",
    "):\n",
    "    model = pickle.load(open(\"models/\" + model_name + \".pkl\", \"rb\"))\n",
    "\n",
    "    result = model.predict(\n",
    "        np.array(\n",
    "            [\n",
    "                [\n",
    "                    tank_liquidlevel,\n",
    "                    flowlevel,\n",
    "                    bottle_liquidlevel,\n",
    "                    motor_status,\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "# import csv file\n",
    "\n",
    "count = 0\n",
    "file_name = \"anomaly.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# df.head()\n",
    "# print(df[\"tank_liquidlevel\"][3])\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "\n",
    "    with open(\"all_test\", \"a\") as f:\n",
    "        print(\n",
    "            get_result(\n",
    "                \"naive_bayes\",\n",
    "                float(df[\"tank_liquidlevel\"][i]),\n",
    "                float(df[\"flowlevel\"][i]),\n",
    "                float(df[\"bottle_liquidlevel\"][i]),\n",
    "                float(df[\"motor_status\"][i]),\n",
    "            )\n",
    "        )\n",
    "        f.write(\n",
    "            str(\n",
    "                get_result(\n",
    "                    \"naive_bayes\",\n",
    "                    float(df[\"tank_liquidlevel\"][i]),\n",
    "                    float(df[\"flowlevel\"][i]),\n",
    "                    float(df[\"bottle_liquidlevel\"][i]),\n",
    "                    float(df[\"motor_status\"][i]),\n",
    "                )\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "    count += 1\n",
    "\n",
    "# algorithm that works\n",
    "\"\"\"\n",
    "1. rf -> works\n",
    "2. gb -> works\n",
    "3. logistic -> works\n",
    "4. nb -> works\n",
    "5. stacking -> works\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"total number of attacks detected by ml: \", count)\n",
    "print(\"total number of attacks undetected by ml: \", len(df) - count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
